{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Import pytorch dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# You cannot change this line.\n",
    "from tools.dataloader import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network module: VGG16\n",
    "class Vgg16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vgg16, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3,64,3) #(in, out, kernel)\n",
    "#         self.conv2 = nn.Conv2d(64,64,3)\n",
    "#         self.pool1 = nn.MaxPool2d(2,2) #(kernel, stride)\n",
    "#         self.conv3 = nn.Conv2d(64,128,3)\n",
    "#         self.conv4 = nn.Conv2d(128,128,3)\n",
    "#         self.pool2 = nn.MaxPool2d(2,2)\n",
    "#         self.conv5 = nn.Conv2d(128,256,3)\n",
    "#         self.conv6 = nn.Conv2d(256,256,3)\n",
    "#         self.fc1 = nn.Linear(1 * 1 * 256, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3,64,3) #(in, out, kernel)\n",
    "        self.conv2 = nn.Conv2d(64,64,3)\n",
    "        self.pool1 = nn.MaxPool2d(2,2) #(kernel, stride)\n",
    "        self.conv3 = nn.Conv2d(64,128,3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.conv4 = nn.Conv2d(128,256,3)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(2 * 2 * 256, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        \n",
    "        #x = F.relu(self.conv6(x))\n",
    "        \n",
    "        \n",
    "        #x = F.relu(self.fc1(x.view(-1, 1 * 1 * 256)))\n",
    "        x = F.relu(self.fc1(x.view(-1, 2 * 2 * 256)))\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([64, 3, 3, 3])\n",
      "conv1.bias torch.Size([64])\n",
      "conv2.weight torch.Size([64, 64, 3, 3])\n",
      "conv2.bias torch.Size([64])\n",
      "conv3.weight torch.Size([128, 64, 3, 3])\n",
      "conv3.bias torch.Size([128])\n",
      "conv4.weight torch.Size([256, 128, 3, 3])\n",
      "conv4.bias torch.Size([256])\n",
      "fc1.weight torch.Size([120, 1024])\n",
      "fc1.bias torch.Size([120])\n",
      "fc2.weight torch.Size([84, 120])\n",
      "fc2.bias torch.Size([84])\n",
      "fc3.weight torch.Size([10, 84])\n",
      "fc3.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Print some parameters for checking\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = Vgg16()\n",
    "net = net.to(device)\n",
    "testArray = torch.zeros([128,3,32,32])\n",
    "x = testArray.to(device)\n",
    "y = net(x)\n",
    "for name, p in net.named_parameters():\n",
    "    print(name, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting some hyperparameters\n",
    "\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 100\n",
    "INITIAL_LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "REG = 1e-4\n",
    "EPOCHS = 300\n",
    "DATAROOT = \"./data\"\n",
    "CHECKPOINT_PATH = \"./saved_model_vgg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop([32, 32], padding=2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.dropbox.com/s/ow0wldxbxmqmtzz/cifar10_trainval.tar.gz?dl=1 to ./data/cifar10_trainval.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141295616it [00:03, 36238153.39it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar10_trainval.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data/cifar10_trainval.tar.gz\n",
      "Extracting ./data/cifar10_trainval.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Call the dataset Loader\n",
    "trainset = CIFAR10(root=DATAROOT, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=1)\n",
    "valset = CIFAR10(root=DATAROOT, train=False, download=True, transform=transform_val)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on GPU...\n"
     ]
    }
   ],
   "source": [
    "# Specify the device for computation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# net = Vgg16()\n",
    "# net = net.to(device)\n",
    "if device =='cuda':\n",
    "    print(\"Train on GPU...\")\n",
    "else:\n",
    "    print(\"Train on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded checkpoint: ./saved_model_vgg/model.h5\n",
      "Starting from epoch 91 \n",
      "Starting from learning rate 0.040288:\n"
     ]
    }
   ],
   "source": [
    "# FLAG for loading the pretrained model\n",
    "TRAIN_FROM_SCRATCH = False\n",
    "# Code for loading checkpoint and recover epoch id.\n",
    "CKPT_PATH = \"./saved_model_vgg/model.h5\"\n",
    "def get_checkpoint(ckpt_path):\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return ckpt\n",
    "\n",
    "ckpt = get_checkpoint(CKPT_PATH)\n",
    "if ckpt is None or TRAIN_FROM_SCRATCH:\n",
    "    if not TRAIN_FROM_SCRATCH:\n",
    "        print(\"Checkpoint not found.\")\n",
    "    print(\"Training from scratch ...\")\n",
    "    start_epoch = 0\n",
    "    current_learning_rate = INITIAL_LR\n",
    "else:\n",
    "    print(\"Successfully loaded checkpoint: %s\" %CKPT_PATH)\n",
    "    net.load_state_dict(ckpt['net'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    current_learning_rate = ckpt['lr']\n",
    "    print(\"Starting from epoch %d \" %start_epoch)\n",
    "\n",
    "print(\"Starting from learning rate %f:\" %current_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss function and specify regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Add optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper variables for graphing\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "validation_accuracies = []\n",
    "epochs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91:\n",
      "352\n",
      "Training loss: 0.9267, Training accuracy: 0.7104\n",
      "Validation loss: 1.0956, Validation accuracy: 0.6650\n",
      "Saving ...\n",
      "Epoch 92:\n",
      "352\n",
      "Training loss: 1.2511, Training accuracy: 0.6156\n",
      "Validation loss: 1.3823, Validation accuracy: 0.5446\n",
      "Current learning rate has decayed to 0.039482\n",
      "Epoch 93:\n",
      "352\n",
      "Training loss: 0.8178, Training accuracy: 0.7561\n",
      "Validation loss: 0.7572, Validation accuracy: 0.7688\n",
      "Saving ...\n",
      "Epoch 94:\n",
      "352\n",
      "Training loss: 0.6277, Training accuracy: 0.8066\n",
      "Validation loss: 0.7117, Validation accuracy: 0.7838\n",
      "Current learning rate has decayed to 0.038692\n",
      "Saving ...\n",
      "Epoch 95:\n",
      "352\n",
      "Training loss: 0.5475, Training accuracy: 0.8309\n",
      "Validation loss: 0.6172, Validation accuracy: 0.8088\n",
      "Saving ...\n",
      "Epoch 96:\n",
      "352\n",
      "Training loss: 0.5068, Training accuracy: 0.8422\n",
      "Validation loss: 0.6586, Validation accuracy: 0.8046\n",
      "Current learning rate has decayed to 0.037919\n",
      "Epoch 97:\n",
      "352\n",
      "Training loss: 0.4666, Training accuracy: 0.8536\n",
      "Validation loss: 0.6020, Validation accuracy: 0.8208\n",
      "Saving ...\n",
      "Epoch 98:\n",
      "352\n",
      "Training loss: 0.4287, Training accuracy: 0.8621\n",
      "Validation loss: 0.5784, Validation accuracy: 0.8218\n",
      "Current learning rate has decayed to 0.037160\n",
      "Saving ...\n",
      "Epoch 99:\n",
      "352\n",
      "Training loss: 0.4011, Training accuracy: 0.8725\n",
      "Validation loss: 0.5989, Validation accuracy: 0.8282\n",
      "Saving ...\n",
      "Epoch 100:\n",
      "352\n",
      "Training loss: 0.3889, Training accuracy: 0.8737\n",
      "Validation loss: 0.5804, Validation accuracy: 0.8260\n",
      "Current learning rate has decayed to 0.036417\n",
      "Epoch 101:\n",
      "352\n",
      "Training loss: 0.3670, Training accuracy: 0.8808\n",
      "Validation loss: 0.5991, Validation accuracy: 0.8272\n",
      "Epoch 102:\n",
      "352\n",
      "Training loss: 0.3566, Training accuracy: 0.8834\n",
      "Validation loss: 0.5660, Validation accuracy: 0.8358\n",
      "Current learning rate has decayed to 0.035689\n",
      "Saving ...\n",
      "Epoch 103:\n",
      "352\n",
      "Training loss: 0.3395, Training accuracy: 0.8876\n",
      "Validation loss: 0.5799, Validation accuracy: 0.8316\n",
      "Epoch 104:\n",
      "352\n",
      "Training loss: 0.3309, Training accuracy: 0.8913\n",
      "Validation loss: 0.5235, Validation accuracy: 0.8526\n",
      "Current learning rate has decayed to 0.034975\n",
      "Saving ...\n",
      "Epoch 105:\n",
      "352\n",
      "Training loss: 0.3139, Training accuracy: 0.8972\n",
      "Validation loss: 0.5748, Validation accuracy: 0.8340\n",
      "Epoch 106:\n",
      "352\n",
      "Training loss: 0.3230, Training accuracy: 0.8936\n",
      "Validation loss: 0.6016, Validation accuracy: 0.8384\n",
      "Current learning rate has decayed to 0.034275\n",
      "Epoch 107:\n",
      "352\n",
      "Training loss: 0.2956, Training accuracy: 0.9036\n",
      "Validation loss: 0.5264, Validation accuracy: 0.8430\n",
      "Epoch 108:\n",
      "352\n",
      "Training loss: 0.2906, Training accuracy: 0.9049\n",
      "Validation loss: 0.5474, Validation accuracy: 0.8470\n",
      "Current learning rate has decayed to 0.033590\n",
      "Epoch 109:\n",
      "352\n",
      "Training loss: 0.2805, Training accuracy: 0.9068\n",
      "Validation loss: 0.5027, Validation accuracy: 0.8500\n",
      "Epoch 110:\n",
      "352\n",
      "Training loss: 0.2674, Training accuracy: 0.9123\n",
      "Validation loss: 0.5284, Validation accuracy: 0.8482\n",
      "Current learning rate has decayed to 0.032918\n",
      "Epoch 111:\n",
      "352\n",
      "Training loss: 0.2582, Training accuracy: 0.9116\n",
      "Validation loss: 0.5185, Validation accuracy: 0.8472\n",
      "Epoch 112:\n",
      "352\n",
      "Training loss: 0.2520, Training accuracy: 0.9175\n",
      "Validation loss: 0.5354, Validation accuracy: 0.8440\n",
      "Current learning rate has decayed to 0.032260\n",
      "Epoch 113:\n",
      "352\n",
      "Training loss: 0.2509, Training accuracy: 0.9174\n",
      "Validation loss: 0.5415, Validation accuracy: 0.8546\n",
      "Saving ...\n",
      "Epoch 114:\n",
      "352\n",
      "Training loss: 0.2386, Training accuracy: 0.9196\n",
      "Validation loss: 0.5603, Validation accuracy: 0.8488\n",
      "Current learning rate has decayed to 0.031614\n",
      "Epoch 115:\n",
      "352\n",
      "Training loss: 0.2377, Training accuracy: 0.9199\n",
      "Validation loss: 0.5381, Validation accuracy: 0.8488\n",
      "Epoch 116:\n",
      "352\n",
      "Training loss: 0.2309, Training accuracy: 0.9238\n",
      "Validation loss: 0.5782, Validation accuracy: 0.8460\n",
      "Current learning rate has decayed to 0.030982\n",
      "Epoch 117:\n",
      "352\n",
      "Training loss: 0.2200, Training accuracy: 0.9258\n",
      "Validation loss: 0.5348, Validation accuracy: 0.8482\n",
      "Epoch 118:\n",
      "352\n",
      "Training loss: 0.2187, Training accuracy: 0.9274\n",
      "Validation loss: 0.5057, Validation accuracy: 0.8628\n",
      "Current learning rate has decayed to 0.030363\n",
      "Saving ...\n",
      "Epoch 119:\n",
      "352\n",
      "Training loss: 0.2214, Training accuracy: 0.9252\n",
      "Validation loss: 0.5704, Validation accuracy: 0.8444\n",
      "Epoch 120:\n",
      "352\n",
      "Training loss: 0.2173, Training accuracy: 0.9281\n",
      "Validation loss: 0.5664, Validation accuracy: 0.8518\n",
      "Current learning rate has decayed to 0.029755\n",
      "Epoch 121:\n",
      "352\n",
      "Training loss: 0.2027, Training accuracy: 0.9308\n",
      "Validation loss: 0.5537, Validation accuracy: 0.8500\n",
      "Epoch 122:\n",
      "352\n",
      "Training loss: 0.2066, Training accuracy: 0.9315\n",
      "Validation loss: 0.5331, Validation accuracy: 0.8476\n",
      "Current learning rate has decayed to 0.029160\n",
      "Epoch 123:\n",
      "352\n",
      "Training loss: 0.1891, Training accuracy: 0.9359\n",
      "Validation loss: 0.5737, Validation accuracy: 0.8522\n",
      "Epoch 124:\n",
      "352\n",
      "Training loss: 0.1973, Training accuracy: 0.9355\n",
      "Validation loss: 0.5421, Validation accuracy: 0.8566\n",
      "Current learning rate has decayed to 0.028577\n",
      "Epoch 125:\n",
      "352\n",
      "Training loss: 0.1879, Training accuracy: 0.9368\n",
      "Validation loss: 0.5117, Validation accuracy: 0.8578\n",
      "Epoch 126:\n",
      "352\n",
      "Training loss: 0.1807, Training accuracy: 0.9374\n",
      "Validation loss: 0.5325, Validation accuracy: 0.8560\n",
      "Current learning rate has decayed to 0.028005\n",
      "Epoch 127:\n",
      "352\n",
      "Training loss: 0.1804, Training accuracy: 0.9394\n",
      "Validation loss: 0.5358, Validation accuracy: 0.8546\n",
      "Epoch 128:\n",
      "352\n",
      "Training loss: 0.1745, Training accuracy: 0.9402\n",
      "Validation loss: 0.5592, Validation accuracy: 0.8536\n",
      "Current learning rate has decayed to 0.027445\n",
      "Epoch 129:\n",
      "352\n",
      "Training loss: 0.1737, Training accuracy: 0.9426\n",
      "Validation loss: 0.5775, Validation accuracy: 0.8498\n",
      "Epoch 130:\n",
      "352\n",
      "Training loss: 0.1646, Training accuracy: 0.9432\n",
      "Validation loss: 0.5453, Validation accuracy: 0.8520\n",
      "Current learning rate has decayed to 0.026896\n",
      "Epoch 131:\n",
      "352\n",
      "Training loss: 0.1697, Training accuracy: 0.9428\n",
      "Validation loss: 0.5514, Validation accuracy: 0.8596\n",
      "Epoch 132:\n",
      "352\n",
      "Training loss: 0.1611, Training accuracy: 0.9436\n",
      "Validation loss: 0.5817, Validation accuracy: 0.8492\n",
      "Current learning rate has decayed to 0.026359\n",
      "Epoch 133:\n",
      "352\n",
      "Training loss: 0.1584, Training accuracy: 0.9483\n",
      "Validation loss: 0.5945, Validation accuracy: 0.8510\n",
      "Epoch 134:\n",
      "352\n",
      "Training loss: 0.1527, Training accuracy: 0.9469\n",
      "Validation loss: 0.5721, Validation accuracy: 0.8530\n",
      "Current learning rate has decayed to 0.025831\n",
      "Epoch 135:\n",
      "352\n",
      "Training loss: 0.1571, Training accuracy: 0.9464\n",
      "Validation loss: 0.5504, Validation accuracy: 0.8534\n",
      "Epoch 136:\n",
      "352\n",
      "Training loss: 0.1501, Training accuracy: 0.9490\n",
      "Validation loss: 0.5753, Validation accuracy: 0.8590\n",
      "Current learning rate has decayed to 0.025315\n",
      "Epoch 137:\n",
      "352\n",
      "Training loss: 0.1434, Training accuracy: 0.9506\n",
      "Validation loss: 0.5714, Validation accuracy: 0.8588\n",
      "Epoch 138:\n",
      "352\n",
      "Training loss: 0.1417, Training accuracy: 0.9534\n",
      "Validation loss: 0.5846, Validation accuracy: 0.8560\n",
      "Current learning rate has decayed to 0.024808\n",
      "Epoch 139:\n",
      "352\n",
      "Training loss: 0.1381, Training accuracy: 0.9530\n",
      "Validation loss: 0.5636, Validation accuracy: 0.8594\n",
      "Epoch 140:\n",
      "352\n",
      "Training loss: 0.1379, Training accuracy: 0.9525\n",
      "Validation loss: 0.5824, Validation accuracy: 0.8586\n",
      "Current learning rate has decayed to 0.024312\n",
      "Epoch 141:\n",
      "352\n",
      "Training loss: 0.1343, Training accuracy: 0.9550\n",
      "Validation loss: 0.5486, Validation accuracy: 0.8598\n",
      "Epoch 142:\n",
      "352\n",
      "Training loss: 0.1313, Training accuracy: 0.9557\n",
      "Validation loss: 0.5510, Validation accuracy: 0.8656\n",
      "Current learning rate has decayed to 0.023826\n",
      "Saving ...\n",
      "Epoch 143:\n",
      "352\n",
      "Training loss: 0.1281, Training accuracy: 0.9574\n",
      "Validation loss: 0.5547, Validation accuracy: 0.8612\n",
      "Epoch 144:\n",
      "352\n",
      "Training loss: 0.1300, Training accuracy: 0.9565\n",
      "Validation loss: 0.5475, Validation accuracy: 0.8610\n",
      "Current learning rate has decayed to 0.023349\n",
      "Epoch 145:\n",
      "352\n",
      "Training loss: 0.1232, Training accuracy: 0.9578\n",
      "Validation loss: 0.5518, Validation accuracy: 0.8632\n",
      "Epoch 146:\n",
      "352\n",
      "Training loss: 0.1223, Training accuracy: 0.9590\n",
      "Validation loss: 0.5447, Validation accuracy: 0.8656\n",
      "Current learning rate has decayed to 0.022883\n",
      "Epoch 147:\n",
      "352\n",
      "Training loss: 0.1231, Training accuracy: 0.9593\n",
      "Validation loss: 0.5551, Validation accuracy: 0.8608\n",
      "Epoch 148:\n",
      "352\n",
      "Training loss: 0.1162, Training accuracy: 0.9602\n",
      "Validation loss: 0.5711, Validation accuracy: 0.8640\n",
      "Current learning rate has decayed to 0.022425\n",
      "Epoch 149:\n",
      "352\n",
      "Training loss: 0.1094, Training accuracy: 0.9617\n",
      "Validation loss: 0.5897, Validation accuracy: 0.8546\n",
      "Epoch 150:\n",
      "352\n",
      "Training loss: 0.1141, Training accuracy: 0.9608\n",
      "Validation loss: 0.5845, Validation accuracy: 0.8666\n",
      "Current learning rate has decayed to 0.021976\n",
      "Saving ...\n",
      "Epoch 151:\n",
      "352\n",
      "Training loss: 0.1071, Training accuracy: 0.9625\n",
      "Validation loss: 0.5708, Validation accuracy: 0.8640\n",
      "Epoch 152:\n",
      "352\n",
      "Training loss: 0.1082, Training accuracy: 0.9629\n",
      "Validation loss: 0.5553, Validation accuracy: 0.8642\n",
      "Current learning rate has decayed to 0.021537\n",
      "Epoch 153:\n",
      "352\n",
      "Training loss: 0.1045, Training accuracy: 0.9647\n",
      "Validation loss: 0.6086, Validation accuracy: 0.8548\n",
      "Epoch 154:\n",
      "352\n",
      "Training loss: 0.1018, Training accuracy: 0.9656\n",
      "Validation loss: 0.5941, Validation accuracy: 0.8592\n",
      "Current learning rate has decayed to 0.021106\n",
      "Epoch 155:\n",
      "352\n",
      "Training loss: 0.1040, Training accuracy: 0.9646\n",
      "Validation loss: 0.5644, Validation accuracy: 0.8678\n",
      "Saving ...\n",
      "Epoch 156:\n",
      "352\n",
      "Training loss: 0.1005, Training accuracy: 0.9664\n",
      "Validation loss: 0.6299, Validation accuracy: 0.8592\n",
      "Current learning rate has decayed to 0.020684\n",
      "Epoch 157:\n",
      "352\n",
      "Training loss: 0.0976, Training accuracy: 0.9664\n",
      "Validation loss: 0.6205, Validation accuracy: 0.8512\n",
      "Epoch 158:\n",
      "352\n",
      "Training loss: 0.0938, Training accuracy: 0.9679\n",
      "Validation loss: 0.5866, Validation accuracy: 0.8616\n",
      "Current learning rate has decayed to 0.020270\n",
      "Epoch 159:\n",
      "352\n",
      "Training loss: 0.0893, Training accuracy: 0.9692\n",
      "Validation loss: 0.5842, Validation accuracy: 0.8650\n",
      "Epoch 160:\n",
      "352\n",
      "Training loss: 0.0942, Training accuracy: 0.9674\n",
      "Validation loss: 0.6257, Validation accuracy: 0.8588\n",
      "Current learning rate has decayed to 0.019865\n",
      "Epoch 161:\n",
      "352\n",
      "Training loss: 0.0883, Training accuracy: 0.9690\n",
      "Validation loss: 0.5827, Validation accuracy: 0.8666\n",
      "Epoch 162:\n",
      "352\n",
      "Training loss: 0.0879, Training accuracy: 0.9694\n",
      "Validation loss: 0.6272, Validation accuracy: 0.8652\n",
      "Current learning rate has decayed to 0.019468\n",
      "Epoch 163:\n",
      "352\n",
      "Training loss: 0.0876, Training accuracy: 0.9698\n",
      "Validation loss: 0.5937, Validation accuracy: 0.8670\n",
      "Epoch 164:\n",
      "352\n",
      "Training loss: 0.0830, Training accuracy: 0.9722\n",
      "Validation loss: 0.6200, Validation accuracy: 0.8588\n",
      "Current learning rate has decayed to 0.019078\n",
      "Epoch 165:\n",
      "352\n",
      "Training loss: 0.0813, Training accuracy: 0.9720\n",
      "Validation loss: 0.6266, Validation accuracy: 0.8630\n",
      "Epoch 166:\n",
      "352\n",
      "Training loss: 0.0837, Training accuracy: 0.9716\n",
      "Validation loss: 0.6163, Validation accuracy: 0.8604\n",
      "Current learning rate has decayed to 0.018697\n",
      "Epoch 167:\n",
      "352\n",
      "Training loss: 0.0846, Training accuracy: 0.9706\n",
      "Validation loss: 0.5895, Validation accuracy: 0.8658\n",
      "Epoch 168:\n",
      "352\n",
      "Training loss: 0.0812, Training accuracy: 0.9720\n",
      "Validation loss: 0.6131, Validation accuracy: 0.8626\n",
      "Current learning rate has decayed to 0.018323\n",
      "Epoch 169:\n",
      "352\n",
      "Training loss: 0.0757, Training accuracy: 0.9746\n",
      "Validation loss: 0.5765, Validation accuracy: 0.8696\n",
      "Saving ...\n",
      "Epoch 170:\n",
      "352\n",
      "Training loss: 0.0784, Training accuracy: 0.9738\n",
      "Validation loss: 0.5962, Validation accuracy: 0.8680\n",
      "Current learning rate has decayed to 0.017956\n",
      "Epoch 171:\n",
      "352\n",
      "Training loss: 0.0740, Training accuracy: 0.9743\n",
      "Validation loss: 0.6141, Validation accuracy: 0.8648\n",
      "Epoch 172:\n",
      "352\n",
      "Training loss: 0.0726, Training accuracy: 0.9751\n",
      "Validation loss: 0.5941, Validation accuracy: 0.8614\n",
      "Current learning rate has decayed to 0.017597\n",
      "Epoch 173:\n",
      "352\n",
      "Training loss: 0.0727, Training accuracy: 0.9752\n",
      "Validation loss: 0.6176, Validation accuracy: 0.8684\n",
      "Epoch 174:\n",
      "352\n",
      "Training loss: 0.0700, Training accuracy: 0.9750\n",
      "Validation loss: 0.5949, Validation accuracy: 0.8690\n",
      "Current learning rate has decayed to 0.017245\n",
      "Epoch 175:\n",
      "352\n",
      "Training loss: 0.0697, Training accuracy: 0.9759\n",
      "Validation loss: 0.5873, Validation accuracy: 0.8704\n",
      "Saving ...\n",
      "Epoch 176:\n",
      "352\n",
      "Training loss: 0.0667, Training accuracy: 0.9769\n",
      "Validation loss: 0.6065, Validation accuracy: 0.8656\n",
      "Current learning rate has decayed to 0.016900\n",
      "Epoch 177:\n",
      "352\n",
      "Training loss: 0.0664, Training accuracy: 0.9771\n",
      "Validation loss: 0.5811, Validation accuracy: 0.8708\n",
      "Saving ...\n",
      "Epoch 178:\n",
      "352\n",
      "Training loss: 0.0663, Training accuracy: 0.9764\n",
      "Validation loss: 0.5950, Validation accuracy: 0.8720\n",
      "Current learning rate has decayed to 0.016562\n",
      "Saving ...\n",
      "Epoch 179:\n",
      "352\n",
      "Training loss: 0.0654, Training accuracy: 0.9771\n",
      "Validation loss: 0.6271, Validation accuracy: 0.8698\n",
      "Epoch 180:\n",
      "352\n",
      "Training loss: 0.0631, Training accuracy: 0.9787\n",
      "Validation loss: 0.6194, Validation accuracy: 0.8684\n",
      "Current learning rate has decayed to 0.016231\n",
      "Epoch 181:\n",
      "352\n",
      "Training loss: 0.0637, Training accuracy: 0.9786\n",
      "Validation loss: 0.5941, Validation accuracy: 0.8696\n",
      "Epoch 182:\n",
      "352\n",
      "Training loss: 0.0622, Training accuracy: 0.9785\n",
      "Validation loss: 0.6092, Validation accuracy: 0.8654\n",
      "Current learning rate has decayed to 0.015906\n",
      "Epoch 183:\n",
      "352\n",
      "Training loss: 0.0603, Training accuracy: 0.9799\n",
      "Validation loss: 0.6226, Validation accuracy: 0.8648\n",
      "Epoch 184:\n",
      "352\n",
      "Training loss: 0.0597, Training accuracy: 0.9793\n",
      "Validation loss: 0.6049, Validation accuracy: 0.8706\n",
      "Current learning rate has decayed to 0.015588\n",
      "Epoch 185:\n",
      "352\n",
      "Training loss: 0.0526, Training accuracy: 0.9815\n",
      "Validation loss: 0.6313, Validation accuracy: 0.8662\n",
      "Epoch 186:\n",
      "352\n",
      "Training loss: 0.0567, Training accuracy: 0.9803\n",
      "Validation loss: 0.6108, Validation accuracy: 0.8746\n",
      "Current learning rate has decayed to 0.015277\n",
      "Saving ...\n",
      "Epoch 187:\n",
      "352\n",
      "Training loss: 0.0538, Training accuracy: 0.9821\n",
      "Validation loss: 0.6413, Validation accuracy: 0.8678\n",
      "Epoch 188:\n",
      "352\n",
      "Training loss: 0.0591, Training accuracy: 0.9796\n",
      "Validation loss: 0.6690, Validation accuracy: 0.8608\n",
      "Current learning rate has decayed to 0.014971\n",
      "Epoch 189:\n",
      "352\n",
      "Training loss: 0.0536, Training accuracy: 0.9820\n",
      "Validation loss: 0.6312, Validation accuracy: 0.8678\n",
      "Epoch 190:\n",
      "352\n",
      "Training loss: 0.0519, Training accuracy: 0.9826\n",
      "Validation loss: 0.5955, Validation accuracy: 0.8746\n",
      "Current learning rate has decayed to 0.014672\n",
      "Epoch 191:\n",
      "352\n",
      "Training loss: 0.0512, Training accuracy: 0.9831\n",
      "Validation loss: 0.6271, Validation accuracy: 0.8740\n",
      "Epoch 192:\n",
      "352\n",
      "Training loss: 0.0490, Training accuracy: 0.9830\n",
      "Validation loss: 0.6278, Validation accuracy: 0.8684\n",
      "Current learning rate has decayed to 0.014378\n",
      "Epoch 193:\n",
      "352\n",
      "Training loss: 0.0476, Training accuracy: 0.9838\n",
      "Validation loss: 0.6493, Validation accuracy: 0.8668\n",
      "Epoch 194:\n",
      "352\n",
      "Training loss: 0.0499, Training accuracy: 0.9831\n",
      "Validation loss: 0.6642, Validation accuracy: 0.8708\n",
      "Current learning rate has decayed to 0.014091\n",
      "Epoch 195:\n",
      "352\n",
      "Training loss: 0.0481, Training accuracy: 0.9833\n",
      "Validation loss: 0.6473, Validation accuracy: 0.8682\n",
      "Epoch 196:\n",
      "352\n",
      "Training loss: 0.0464, Training accuracy: 0.9842\n",
      "Validation loss: 0.6710, Validation accuracy: 0.8702\n",
      "Current learning rate has decayed to 0.013809\n",
      "Epoch 197:\n",
      "352\n",
      "Training loss: 0.0482, Training accuracy: 0.9836\n",
      "Validation loss: 0.6140, Validation accuracy: 0.8712\n",
      "Epoch 198:\n",
      "352\n",
      "Training loss: 0.0451, Training accuracy: 0.9842\n",
      "Validation loss: 0.6681, Validation accuracy: 0.8680\n",
      "Current learning rate has decayed to 0.013533\n",
      "Epoch 199:\n",
      "352\n",
      "Training loss: 0.0413, Training accuracy: 0.9856\n",
      "Validation loss: 0.6703, Validation accuracy: 0.8710\n",
      "Optimization finished.\n"
     ]
    }
   ],
   "source": [
    "# Start the training/validation process\n",
    "# The process should take about 5 minutes on a GTX 1070-Ti\n",
    "# if the code is written efficiently.\n",
    "global_step = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "for i in range(start_epoch, EPOCHS):\n",
    "    #print(datetime.datetime.now())\n",
    "    # Switch to train mode\n",
    "    net.train()\n",
    "    print(\"Epoch %d:\" %i)\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    # Train the training dataset for 1 epoch.\n",
    "    print(len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # Copy inputs to device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # Printing initial loss \n",
    "        if i == 0 and batch_idx == 0:\n",
    "            print(\"initial loss: \",  loss)\n",
    "        \n",
    "        # Now backward loss\n",
    "        loss.backward()\n",
    "        # Apply gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate predicted labels\n",
    "        _, predicted = outputs.max(1)\n",
    "        # Calculate accuracy\n",
    "        total_examples += inputs.size(0)\n",
    "        correct_examples += torch.eq(targets, predicted).sum().item()\n",
    "\n",
    "        train_loss += loss\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 100 == 0:\n",
    "            avg_loss = train_loss / (batch_idx + 1)\n",
    "        pass\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    \n",
    "    #print(datetime.datetime.now())\n",
    "    # Validate on the validation dataset\n",
    "    #print(\"Validation...\")\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    # Disable gradient during validation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # Copy inputs to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Generate output from the DNN.\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Calculate predicted labels\n",
    "            _, predicted = outputs.max(1)\n",
    "            # Calculate accuracy\n",
    "            total_examples += inputs.size(0)\n",
    "            correct_examples += torch.eq(targets, predicted).sum().item()\n",
    "            val_loss += loss\n",
    "\n",
    "    avg_loss = val_loss / len(valloader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "    \n",
    "    # For graphing\n",
    "    val_losses.append(avg_loss)\n",
    "    epochs.append(i)\n",
    "    validation_accuracies.append(avg_acc)\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    Assignment 4(b)\n",
    "    Learning rate is an important hyperparameter to tune. Specify a \n",
    "    learning rate decay policy and apply it in your training process. \n",
    "    Briefly describe its impact on the learning curveduring your \n",
    "    training process.    \n",
    "    Reference learning rate schedule: \n",
    "    decay 0.98 for every 2 epochs. You may tune this parameter but \n",
    "    minimal gain will be achieved.\n",
    "    Assignment 4(c)\n",
    "    As we can see from above, hyperparameter optimization is critical \n",
    "    to obtain a good performance of DNN models. Try to fine-tune the \n",
    "    model to over 70% accuracy. You may also increase the number of \n",
    "    epochs to up to 100 during the process. Briefly describe what you \n",
    "    have tried to improve the performance of the LeNet-5 model.\n",
    "    \"\"\"\n",
    "    DECAY_EPOCHS = 2\n",
    "    DECAY = 0.98\n",
    "    if i % DECAY_EPOCHS == 0 and i != 0:\n",
    "        current_learning_rate *= DECAY\n",
    "        for param_group in optimizer.param_groups:\n",
    "            # Assign the learning rate parameter\n",
    "            param_group['lr'] = current_learning_rate\n",
    "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
    "    \n",
    "    # Save for checkpoint\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "        if not os.path.exists(CHECKPOINT_PATH):\n",
    "            os.makedirs(CHECKPOINT_PATH)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'net': net.state_dict(),\n",
    "                 'epoch': i,\n",
    "                 'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_PATH, 'model.h5'))\n",
    "\n",
    "print(\"Optimization finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max validation acc: 0.8462  at Epoch:  90\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "tmp = []\n",
    "for i in val_losses:\n",
    "    tmp.append(i)\n",
    "val_losses = tmp\n",
    "\n",
    "tmp = []\n",
    "for i in train_losses:\n",
    "    tmp.append(i)\n",
    "train_losses = tmp\n",
    "\n",
    "epochs = []\n",
    "for i in range(len(val_losses)):\n",
    "    epochs.append(i)\n",
    "\n",
    "    \n",
    "val, idx = max((val, idx) for (idx, val) in enumerate(validation_accuracies))\n",
    "print(\"Max validation acc:\", val, \" at Epoch: \", idx)\n",
    "    \n",
    "#plt.plot(epochs, val_losses, label=\"val_losses\") \n",
    "plt.plot(epochs, train_losses, label=\"train_losses\") \n",
    "plt.legend()\n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Loss') \n",
    "plt.title('Loss curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.testloader import TEST_SET\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "testset = TEST_SET(root=DATAROOT, train=False, transform=transform_train)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=1)\n",
    "\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # Copy inputs to device         \n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output from the DNN.\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # Calculate predicted labels\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        predicted_np = predicted.data.cpu().numpy()\n",
    "        for i in predicted_np:\n",
    "            results.append(i)\n",
    "# len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"Id\", \"Category\"]\n",
    "\n",
    "out_to_file = []\n",
    "\n",
    "out_to_file.append(header)\n",
    "\n",
    "for i in range(len(results)):\n",
    "    tmp = [i, results[i]]\n",
    "    out_to_file.append(tmp)\n",
    "\n",
    "#print(out_to_file)\n",
    "\n",
    "np_out = np.array(out_to_file)\n",
    "\n",
    "np.savetxt('output.csv', np_out, fmt='%s', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
