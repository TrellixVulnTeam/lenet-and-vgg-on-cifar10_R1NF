{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Import pytorch dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# You cannot change this line.\n",
    "from tools.dataloader import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network module: VGG16\n",
    "class Vgg16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vgg16, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3,64,3) #(in, out, kernel)\n",
    "#         self.conv2 = nn.Conv2d(64,64,3)\n",
    "#         self.pool1 = nn.MaxPool2d(2,2) #(kernel, stride)\n",
    "#         self.conv3 = nn.Conv2d(64,128,3)\n",
    "#         self.conv4 = nn.Conv2d(128,128,3)\n",
    "#         self.pool2 = nn.MaxPool2d(2,2)\n",
    "#         self.conv5 = nn.Conv2d(128,256,3)\n",
    "#         self.conv6 = nn.Conv2d(256,256,3)\n",
    "#         self.fc1 = nn.Linear(1 * 1 * 256, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3,64,3) #(in, out, kernel)\n",
    "        self.conv2 = nn.Conv2d(64,64,3)\n",
    "        self.pool1 = nn.MaxPool2d(2,2) #(kernel, stride)\n",
    "        self.conv3 = nn.Conv2d(64,128,3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.conv4 = nn.Conv2d(128,256,3)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(2 * 2 * 256, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        \n",
    "        #x = F.relu(self.conv6(x))\n",
    "        \n",
    "        \n",
    "        #x = F.relu(self.fc1(x.view(-1, 1 * 1 * 256)))\n",
    "        x = F.relu(self.fc1(x.view(-1, 2 * 2 * 256)))\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([64, 3, 3, 3])\n",
      "conv1.bias torch.Size([64])\n",
      "conv2.weight torch.Size([64, 64, 3, 3])\n",
      "conv2.bias torch.Size([64])\n",
      "conv3.weight torch.Size([128, 64, 3, 3])\n",
      "conv3.bias torch.Size([128])\n",
      "conv4.weight torch.Size([256, 128, 3, 3])\n",
      "conv4.bias torch.Size([256])\n",
      "fc1.weight torch.Size([120, 1024])\n",
      "fc1.bias torch.Size([120])\n",
      "fc2.weight torch.Size([84, 120])\n",
      "fc2.bias torch.Size([84])\n",
      "fc3.weight torch.Size([10, 84])\n",
      "fc3.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Print some parameters for checking\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = Vgg16()\n",
    "net = net.to(device)\n",
    "testArray = torch.zeros([128,3,32,32])\n",
    "x = testArray.to(device)\n",
    "y = net(x)\n",
    "for name, p in net.named_parameters():\n",
    "    print(name, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting some hyperparameters\n",
    "\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 100\n",
    "INITIAL_LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "REG = 1e-4\n",
    "EPOCHS = 200\n",
    "DATAROOT = \"./data\"\n",
    "CHECKPOINT_PATH = \"./saved_model_vgg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop([32, 32], padding=2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/cifar10_trainval.tar.gz\n",
      "Extracting ./data/cifar10_trainval.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data/cifar10_trainval.tar.gz\n",
      "Extracting ./data/cifar10_trainval.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Call the dataset Loader\n",
    "trainset = CIFAR10(root=DATAROOT, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=1)\n",
    "valset = CIFAR10(root=DATAROOT, train=False, download=True, transform=transform_val)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on GPU...\n"
     ]
    }
   ],
   "source": [
    "# Specify the device for computation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# net = Vgg16()\n",
    "# net = net.to(device)\n",
    "if device =='cuda':\n",
    "    print(\"Train on GPU...\")\n",
    "else:\n",
    "    print(\"Train on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training from scratch ...\n",
      "Starting from learning rate 0.100000:\n"
     ]
    }
   ],
   "source": [
    "# FLAG for loading the pretrained model\n",
    "TRAIN_FROM_SCRATCH = False\n",
    "# Code for loading checkpoint and recover epoch id.\n",
    "CKPT_PATH = \"./saved_model_vgg/model.h5\"\n",
    "def get_checkpoint(ckpt_path):\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return ckpt\n",
    "\n",
    "ckpt = get_checkpoint(CKPT_PATH)\n",
    "if ckpt is None or TRAIN_FROM_SCRATCH:\n",
    "    if not TRAIN_FROM_SCRATCH:\n",
    "        print(\"Checkpoint not found.\")\n",
    "    print(\"Training from scratch ...\")\n",
    "    start_epoch = 0\n",
    "    current_learning_rate = INITIAL_LR\n",
    "else:\n",
    "    print(\"Successfully loaded checkpoint: %s\" %CKPT_PATH)\n",
    "    net.load_state_dict(ckpt['net'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    current_learning_rate = ckpt['lr']\n",
    "    print(\"Starting from epoch %d \" %start_epoch)\n",
    "\n",
    "print(\"Starting from learning rate %f:\" %current_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss function and specify regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Add optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper variables for graphing\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "validation_accuracies = []\n",
    "epochs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "352\n",
      "initial loss:  tensor(2.3063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Training loss: 1.8789, Training accuracy: 0.3203\n",
      "Validation loss: 1.5202, Validation accuracy: 0.4538\n",
      "Saving ...\n",
      "Epoch 1:\n",
      "352\n",
      "Training loss: 1.3933, Training accuracy: 0.5017\n",
      "Validation loss: 1.2482, Validation accuracy: 0.5544\n",
      "Saving ...\n",
      "Epoch 2:\n",
      "352\n",
      "Training loss: 1.2346, Training accuracy: 0.5692\n",
      "Validation loss: 1.1899, Validation accuracy: 0.5820\n",
      "Current learning rate has decayed to 0.098000\n",
      "Saving ...\n",
      "Epoch 3:\n",
      "352\n",
      "Training loss: 1.2116, Training accuracy: 0.5844\n",
      "Validation loss: 1.1071, Validation accuracy: 0.6404\n",
      "Saving ...\n",
      "Epoch 4:\n",
      "352\n",
      "Training loss: 1.1726, Training accuracy: 0.6008\n",
      "Validation loss: 1.1297, Validation accuracy: 0.6094\n",
      "Current learning rate has decayed to 0.096040\n",
      "Epoch 5:\n",
      "352\n",
      "Training loss: 1.1660, Training accuracy: 0.6147\n",
      "Validation loss: 1.2847, Validation accuracy: 0.5920\n",
      "Epoch 6:\n",
      "352\n",
      "Training loss: 1.1672, Training accuracy: 0.6163\n",
      "Validation loss: 1.1174, Validation accuracy: 0.6288\n",
      "Current learning rate has decayed to 0.094119\n",
      "Epoch 7:\n",
      "352\n",
      "Training loss: 1.1282, Training accuracy: 0.6251\n",
      "Validation loss: 1.1256, Validation accuracy: 0.6254\n",
      "Epoch 8:\n",
      "352\n",
      "Training loss: 1.1495, Training accuracy: 0.6165\n",
      "Validation loss: 1.1710, Validation accuracy: 0.6072\n",
      "Current learning rate has decayed to 0.092237\n",
      "Epoch 9:\n",
      "352\n",
      "Training loss: 1.1546, Training accuracy: 0.6217\n",
      "Validation loss: 1.3429, Validation accuracy: 0.6056\n",
      "Epoch 10:\n",
      "352\n",
      "Training loss: 1.1246, Training accuracy: 0.6329\n",
      "Validation loss: 1.0830, Validation accuracy: 0.6432\n",
      "Current learning rate has decayed to 0.090392\n",
      "Saving ...\n",
      "Epoch 11:\n",
      "352\n",
      "Training loss: 1.1272, Training accuracy: 0.6360\n",
      "Validation loss: 1.0470, Validation accuracy: 0.6644\n",
      "Saving ...\n",
      "Epoch 12:\n",
      "352\n",
      "Training loss: 1.1397, Training accuracy: 0.6285\n",
      "Validation loss: 1.1189, Validation accuracy: 0.6398\n",
      "Current learning rate has decayed to 0.088584\n",
      "Epoch 13:\n",
      "352\n",
      "Training loss: 1.1146, Training accuracy: 0.6404\n",
      "Validation loss: 1.2462, Validation accuracy: 0.5952\n",
      "Epoch 14:\n",
      "352\n",
      "Training loss: 1.1673, Training accuracy: 0.6254\n",
      "Validation loss: 1.1657, Validation accuracy: 0.6238\n",
      "Current learning rate has decayed to 0.086813\n",
      "Epoch 15:\n",
      "352\n",
      "Training loss: 1.1637, Training accuracy: 0.6296\n",
      "Validation loss: 1.1551, Validation accuracy: 0.6278\n",
      "Epoch 16:\n",
      "352\n",
      "Training loss: 1.1492, Training accuracy: 0.6309\n",
      "Validation loss: 1.3450, Validation accuracy: 0.5896\n",
      "Current learning rate has decayed to 0.085076\n",
      "Epoch 17:\n",
      "352\n",
      "Training loss: 1.1803, Training accuracy: 0.6268\n",
      "Validation loss: 1.1171, Validation accuracy: 0.6380\n",
      "Epoch 18:\n",
      "352\n",
      "Training loss: 1.1526, Training accuracy: 0.6324\n",
      "Validation loss: 1.3259, Validation accuracy: 0.5814\n",
      "Current learning rate has decayed to 0.083375\n",
      "Epoch 19:\n",
      "352\n",
      "Training loss: 1.1865, Training accuracy: 0.6248\n",
      "Validation loss: 1.1262, Validation accuracy: 0.6328\n",
      "Epoch 20:\n",
      "352\n",
      "Training loss: 1.1432, Training accuracy: 0.6397\n",
      "Validation loss: 1.1034, Validation accuracy: 0.6410\n",
      "Current learning rate has decayed to 0.081707\n",
      "Epoch 21:\n",
      "352\n",
      "Training loss: 1.1106, Training accuracy: 0.6481\n",
      "Validation loss: 1.0584, Validation accuracy: 0.6596\n",
      "Epoch 22:\n",
      "352\n",
      "Training loss: 1.1020, Training accuracy: 0.6516\n",
      "Validation loss: 1.0991, Validation accuracy: 0.6524\n",
      "Current learning rate has decayed to 0.080073\n",
      "Epoch 23:\n",
      "352\n",
      "Training loss: 1.1301, Training accuracy: 0.6490\n",
      "Validation loss: 1.1228, Validation accuracy: 0.6576\n",
      "Epoch 24:\n",
      "352\n",
      "Training loss: 1.1605, Training accuracy: 0.6352\n",
      "Validation loss: 1.0960, Validation accuracy: 0.6512\n",
      "Current learning rate has decayed to 0.078472\n",
      "Epoch 25:\n",
      "352\n",
      "Training loss: 1.1744, Training accuracy: 0.6326\n",
      "Validation loss: 1.2073, Validation accuracy: 0.6204\n",
      "Epoch 26:\n",
      "352\n",
      "Training loss: 1.0983, Training accuracy: 0.6557\n",
      "Validation loss: 1.0640, Validation accuracy: 0.6670\n",
      "Current learning rate has decayed to 0.076902\n",
      "Saving ...\n",
      "Epoch 27:\n",
      "352\n",
      "Training loss: 1.0685, Training accuracy: 0.6622\n",
      "Validation loss: 1.1545, Validation accuracy: 0.6524\n",
      "Epoch 28:\n",
      "352\n",
      "Training loss: 1.0831, Training accuracy: 0.6583\n",
      "Validation loss: 1.1666, Validation accuracy: 0.6204\n",
      "Current learning rate has decayed to 0.075364\n",
      "Epoch 29:\n",
      "352\n",
      "Training loss: 1.0168, Training accuracy: 0.6798\n",
      "Validation loss: 1.1190, Validation accuracy: 0.6480\n",
      "Epoch 30:\n",
      "352\n",
      "Training loss: 1.0497, Training accuracy: 0.6733\n",
      "Validation loss: 1.0981, Validation accuracy: 0.6362\n",
      "Current learning rate has decayed to 0.073857\n",
      "Epoch 31:\n",
      "352\n",
      "Training loss: 1.0178, Training accuracy: 0.6825\n",
      "Validation loss: 1.0129, Validation accuracy: 0.6858\n",
      "Saving ...\n",
      "Epoch 32:\n",
      "352\n",
      "Training loss: 0.9829, Training accuracy: 0.6920\n",
      "Validation loss: 1.0097, Validation accuracy: 0.6902\n",
      "Current learning rate has decayed to 0.072380\n",
      "Saving ...\n",
      "Epoch 33:\n",
      "352\n",
      "Training loss: 0.9346, Training accuracy: 0.7049\n",
      "Validation loss: 0.9163, Validation accuracy: 0.7136\n",
      "Saving ...\n",
      "Epoch 34:\n",
      "352\n",
      "Training loss: 0.9641, Training accuracy: 0.6987\n",
      "Validation loss: 0.9250, Validation accuracy: 0.7160\n",
      "Current learning rate has decayed to 0.070932\n",
      "Saving ...\n",
      "Epoch 35:\n",
      "352\n",
      "Training loss: 0.9227, Training accuracy: 0.7094\n",
      "Validation loss: 0.9515, Validation accuracy: 0.6898\n",
      "Epoch 36:\n",
      "352\n",
      "Training loss: 0.9212, Training accuracy: 0.7133\n",
      "Validation loss: 0.9383, Validation accuracy: 0.7106\n",
      "Current learning rate has decayed to 0.069514\n",
      "Epoch 37:\n",
      "352\n",
      "Training loss: 0.8907, Training accuracy: 0.7209\n",
      "Validation loss: 0.9243, Validation accuracy: 0.7156\n",
      "Epoch 38:\n",
      "352\n",
      "Training loss: 0.8551, Training accuracy: 0.7314\n",
      "Validation loss: 0.9186, Validation accuracy: 0.7238\n",
      "Current learning rate has decayed to 0.068123\n",
      "Saving ...\n",
      "Epoch 39:\n",
      "352\n",
      "Training loss: 0.8696, Training accuracy: 0.7223\n",
      "Validation loss: 0.9264, Validation accuracy: 0.7064\n",
      "Epoch 40:\n",
      "352\n",
      "Training loss: 0.8205, Training accuracy: 0.7394\n",
      "Validation loss: 0.9329, Validation accuracy: 0.7060\n",
      "Current learning rate has decayed to 0.066761\n",
      "Epoch 41:\n",
      "352\n",
      "Training loss: 0.8075, Training accuracy: 0.7444\n",
      "Validation loss: 0.9066, Validation accuracy: 0.7280\n",
      "Saving ...\n",
      "Epoch 42:\n",
      "352\n",
      "Training loss: 0.8148, Training accuracy: 0.7449\n",
      "Validation loss: 0.7986, Validation accuracy: 0.7396\n",
      "Current learning rate has decayed to 0.065426\n",
      "Saving ...\n",
      "Epoch 43:\n",
      "352\n",
      "Training loss: 0.7740, Training accuracy: 0.7534\n",
      "Validation loss: 0.8578, Validation accuracy: 0.7288\n",
      "Epoch 44:\n",
      "352\n",
      "Training loss: 0.7673, Training accuracy: 0.7599\n",
      "Validation loss: 0.8088, Validation accuracy: 0.7456\n",
      "Current learning rate has decayed to 0.064117\n",
      "Saving ...\n",
      "Epoch 45:\n",
      "352\n",
      "Training loss: 0.7430, Training accuracy: 0.7656\n",
      "Validation loss: 0.8496, Validation accuracy: 0.7430\n",
      "Epoch 46:\n",
      "352\n",
      "Training loss: 0.7371, Training accuracy: 0.7702\n",
      "Validation loss: 0.7538, Validation accuracy: 0.7736\n",
      "Current learning rate has decayed to 0.062835\n",
      "Saving ...\n",
      "Epoch 47:\n",
      "352\n",
      "Training loss: 0.7190, Training accuracy: 0.7712\n",
      "Validation loss: 0.8861, Validation accuracy: 0.7318\n",
      "Epoch 48:\n",
      "352\n",
      "Training loss: 0.7090, Training accuracy: 0.7775\n",
      "Validation loss: 0.7968, Validation accuracy: 0.7524\n",
      "Current learning rate has decayed to 0.061578\n",
      "Epoch 49:\n",
      "352\n",
      "Training loss: 0.6959, Training accuracy: 0.7829\n",
      "Validation loss: 0.7554, Validation accuracy: 0.7590\n",
      "Epoch 50:\n",
      "352\n",
      "Training loss: 0.6730, Training accuracy: 0.7872\n",
      "Validation loss: 0.7245, Validation accuracy: 0.7720\n",
      "Current learning rate has decayed to 0.060346\n",
      "Epoch 51:\n",
      "352\n",
      "Training loss: 0.6704, Training accuracy: 0.7919\n",
      "Validation loss: 0.7649, Validation accuracy: 0.7678\n",
      "Epoch 52:\n",
      "352\n",
      "Training loss: 0.6689, Training accuracy: 0.7884\n",
      "Validation loss: 0.7684, Validation accuracy: 0.7496\n",
      "Current learning rate has decayed to 0.059140\n",
      "Epoch 53:\n",
      "352\n",
      "Training loss: 0.6288, Training accuracy: 0.8010\n",
      "Validation loss: 0.7545, Validation accuracy: 0.7762\n",
      "Saving ...\n",
      "Epoch 54:\n",
      "352\n",
      "Training loss: 0.6315, Training accuracy: 0.7968\n",
      "Validation loss: 0.7175, Validation accuracy: 0.7792\n",
      "Current learning rate has decayed to 0.057957\n",
      "Saving ...\n",
      "Epoch 55:\n",
      "352\n",
      "Training loss: 0.6313, Training accuracy: 0.8007\n",
      "Validation loss: 0.7167, Validation accuracy: 0.7892\n",
      "Saving ...\n",
      "Epoch 56:\n",
      "352\n",
      "Training loss: 0.6045, Training accuracy: 0.8080\n",
      "Validation loss: 0.7361, Validation accuracy: 0.7812\n",
      "Current learning rate has decayed to 0.056798\n",
      "Epoch 57:\n",
      "352\n",
      "Training loss: 0.6079, Training accuracy: 0.8076\n",
      "Validation loss: 0.6737, Validation accuracy: 0.7848\n",
      "Epoch 58:\n",
      "352\n",
      "Training loss: 0.5781, Training accuracy: 0.8168\n",
      "Validation loss: 0.6597, Validation accuracy: 0.7982\n",
      "Current learning rate has decayed to 0.055662\n",
      "Saving ...\n",
      "Epoch 59:\n",
      "352\n",
      "Training loss: 0.5570, Training accuracy: 0.8208\n",
      "Validation loss: 0.7040, Validation accuracy: 0.7776\n",
      "Epoch 60:\n",
      "352\n",
      "Training loss: 0.5537, Training accuracy: 0.8229\n",
      "Validation loss: 0.6619, Validation accuracy: 0.7914\n",
      "Current learning rate has decayed to 0.054548\n",
      "Epoch 61:\n",
      "352\n",
      "Training loss: 0.5423, Training accuracy: 0.8268\n",
      "Validation loss: 0.6558, Validation accuracy: 0.8120\n",
      "Saving ...\n",
      "Epoch 62:\n",
      "352\n",
      "Training loss: 0.5320, Training accuracy: 0.8293\n",
      "Validation loss: 0.6513, Validation accuracy: 0.7950\n",
      "Current learning rate has decayed to 0.053457\n",
      "Epoch 63:\n",
      "352\n",
      "Training loss: 0.5172, Training accuracy: 0.8330\n",
      "Validation loss: 0.6533, Validation accuracy: 0.8066\n",
      "Epoch 64:\n",
      "352\n",
      "Training loss: 0.5150, Training accuracy: 0.8350\n",
      "Validation loss: 0.6504, Validation accuracy: 0.7986\n",
      "Current learning rate has decayed to 0.052388\n",
      "Epoch 65:\n",
      "352\n",
      "Training loss: 0.4949, Training accuracy: 0.8408\n",
      "Validation loss: 0.6835, Validation accuracy: 0.7934\n",
      "Epoch 66:\n",
      "352\n",
      "Training loss: 0.5012, Training accuracy: 0.8386\n",
      "Validation loss: 0.6318, Validation accuracy: 0.8084\n",
      "Current learning rate has decayed to 0.051341\n",
      "Epoch 67:\n",
      "352\n",
      "Training loss: 0.4810, Training accuracy: 0.8442\n",
      "Validation loss: 0.6535, Validation accuracy: 0.8084\n",
      "Epoch 68:\n",
      "352\n",
      "Training loss: 0.4854, Training accuracy: 0.8413\n",
      "Validation loss: 0.6835, Validation accuracy: 0.8064\n",
      "Current learning rate has decayed to 0.050314\n",
      "Epoch 69:\n",
      "352\n",
      "Training loss: 0.4792, Training accuracy: 0.8461\n",
      "Validation loss: 0.6418, Validation accuracy: 0.8080\n",
      "Epoch 70:\n",
      "352\n",
      "Training loss: 0.4672, Training accuracy: 0.8489\n",
      "Validation loss: 0.6509, Validation accuracy: 0.8092\n",
      "Current learning rate has decayed to 0.049307\n",
      "Epoch 71:\n",
      "352\n",
      "Training loss: 0.4557, Training accuracy: 0.8536\n",
      "Validation loss: 0.6644, Validation accuracy: 0.8038\n",
      "Epoch 72:\n",
      "352\n",
      "Training loss: 0.4367, Training accuracy: 0.8596\n",
      "Validation loss: 0.6121, Validation accuracy: 0.8110\n",
      "Current learning rate has decayed to 0.048321\n",
      "Epoch 73:\n",
      "352\n",
      "Training loss: 0.4252, Training accuracy: 0.8631\n",
      "Validation loss: 0.5839, Validation accuracy: 0.8226\n",
      "Saving ...\n",
      "Epoch 74:\n",
      "352\n",
      "Training loss: 0.4255, Training accuracy: 0.8626\n",
      "Validation loss: 0.6010, Validation accuracy: 0.8146\n",
      "Current learning rate has decayed to 0.047355\n",
      "Epoch 75:\n",
      "352\n",
      "Training loss: 0.4107, Training accuracy: 0.8686\n",
      "Validation loss: 0.5853, Validation accuracy: 0.8240\n",
      "Saving ...\n",
      "Epoch 76:\n",
      "352\n",
      "Training loss: 0.4132, Training accuracy: 0.8654\n",
      "Validation loss: 0.6418, Validation accuracy: 0.8152\n",
      "Current learning rate has decayed to 0.046408\n",
      "Epoch 77:\n",
      "352\n",
      "Training loss: 0.3961, Training accuracy: 0.8723\n",
      "Validation loss: 0.6245, Validation accuracy: 0.8140\n",
      "Epoch 78:\n",
      "352\n",
      "Training loss: 0.3920, Training accuracy: 0.8734\n",
      "Validation loss: 0.5964, Validation accuracy: 0.8246\n",
      "Current learning rate has decayed to 0.045480\n",
      "Saving ...\n",
      "Epoch 79:\n",
      "352\n",
      "Training loss: 0.3832, Training accuracy: 0.8745\n",
      "Validation loss: 0.6073, Validation accuracy: 0.8178\n",
      "Epoch 80:\n",
      "352\n",
      "Training loss: 0.3806, Training accuracy: 0.8758\n",
      "Validation loss: 0.6162, Validation accuracy: 0.8208\n",
      "Current learning rate has decayed to 0.044570\n",
      "Epoch 81:\n",
      "352\n",
      "Training loss: 0.3725, Training accuracy: 0.8775\n",
      "Validation loss: 0.5680, Validation accuracy: 0.8322\n",
      "Saving ...\n",
      "Epoch 82:\n",
      "352\n",
      "Training loss: 0.3650, Training accuracy: 0.8810\n",
      "Validation loss: 0.5730, Validation accuracy: 0.8332\n",
      "Current learning rate has decayed to 0.043679\n",
      "Saving ...\n",
      "Epoch 83:\n",
      "352\n",
      "Training loss: 0.3446, Training accuracy: 0.8860\n",
      "Validation loss: 0.5985, Validation accuracy: 0.8280\n",
      "Epoch 84:\n",
      "352\n",
      "Training loss: 0.3501, Training accuracy: 0.8868\n",
      "Validation loss: 0.5921, Validation accuracy: 0.8322\n",
      "Current learning rate has decayed to 0.042805\n",
      "Epoch 85:\n",
      "352\n",
      "Training loss: 0.3316, Training accuracy: 0.8891\n",
      "Validation loss: 0.6139, Validation accuracy: 0.8250\n",
      "Epoch 86:\n",
      "352\n",
      "Training loss: 0.3318, Training accuracy: 0.8931\n",
      "Validation loss: 0.5760, Validation accuracy: 0.8310\n",
      "Current learning rate has decayed to 0.041949\n",
      "Epoch 87:\n",
      "352\n",
      "Training loss: 0.3237, Training accuracy: 0.8919\n",
      "Validation loss: 0.6151, Validation accuracy: 0.8274\n",
      "Epoch 88:\n",
      "352\n",
      "Training loss: 0.3236, Training accuracy: 0.8959\n",
      "Validation loss: 0.5508, Validation accuracy: 0.8312\n",
      "Current learning rate has decayed to 0.041110\n",
      "Epoch 89:\n",
      "352\n",
      "Training loss: 0.3179, Training accuracy: 0.8956\n",
      "Validation loss: 0.5677, Validation accuracy: 0.8352\n",
      "Saving ...\n",
      "Epoch 90:\n",
      "352\n",
      "Training loss: 0.3092, Training accuracy: 0.8972\n",
      "Validation loss: 0.5608, Validation accuracy: 0.8462\n",
      "Current learning rate has decayed to 0.040288\n",
      "Saving ...\n",
      "Epoch 91:\n",
      "352\n",
      "Training loss: 0.3006, Training accuracy: 0.9001\n",
      "Validation loss: 0.5215, Validation accuracy: 0.8438\n",
      "Epoch 92:\n",
      "352\n",
      "Training loss: 0.3042, Training accuracy: 0.9014\n",
      "Validation loss: 0.5825, Validation accuracy: 0.8390\n",
      "Current learning rate has decayed to 0.039482\n",
      "Epoch 93:\n",
      "352\n",
      "Training loss: 0.2883, Training accuracy: 0.9048\n",
      "Validation loss: 0.5529, Validation accuracy: 0.8440\n",
      "Epoch 94:\n",
      "352\n",
      "Training loss: 0.2882, Training accuracy: 0.9056\n",
      "Validation loss: 0.5751, Validation accuracy: 0.8352\n",
      "Current learning rate has decayed to 0.038692\n",
      "Epoch 95:\n",
      "352\n",
      "Training loss: 0.2789, Training accuracy: 0.9070\n",
      "Validation loss: 0.5484, Validation accuracy: 0.8434\n",
      "Epoch 96:\n",
      "352\n",
      "Training loss: 0.2706, Training accuracy: 0.9078\n",
      "Validation loss: 0.5953, Validation accuracy: 0.8304\n",
      "Current learning rate has decayed to 0.037919\n",
      "Epoch 97:\n",
      "352\n",
      "Training loss: 0.2711, Training accuracy: 0.9104\n",
      "Validation loss: 0.5663, Validation accuracy: 0.8360\n",
      "Epoch 98:\n",
      "352\n",
      "Training loss: 0.2601, Training accuracy: 0.9115\n",
      "Validation loss: 0.5778, Validation accuracy: 0.8418\n",
      "Current learning rate has decayed to 0.037160\n",
      "Epoch 99:\n",
      "352\n",
      "Training loss: 0.2603, Training accuracy: 0.9136\n",
      "Validation loss: 0.5587, Validation accuracy: 0.8378\n",
      "Optimization finished.\n"
     ]
    }
   ],
   "source": [
    "# Start the training/validation process\n",
    "# The process should take about 5 minutes on a GTX 1070-Ti\n",
    "# if the code is written efficiently.\n",
    "global_step = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "for i in range(start_epoch, EPOCHS):\n",
    "    #print(datetime.datetime.now())\n",
    "    # Switch to train mode\n",
    "    net.train()\n",
    "    print(\"Epoch %d:\" %i)\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    # Train the training dataset for 1 epoch.\n",
    "    print(len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # Copy inputs to device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # Printing initial loss \n",
    "        if i == 0 and batch_idx == 0:\n",
    "            print(\"initial loss: \",  loss)\n",
    "        \n",
    "        # Now backward loss\n",
    "        loss.backward()\n",
    "        # Apply gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate predicted labels\n",
    "        _, predicted = outputs.max(1)\n",
    "        # Calculate accuracy\n",
    "        total_examples += inputs.size(0)\n",
    "        correct_examples += torch.eq(targets, predicted).sum().item()\n",
    "\n",
    "        train_loss += loss\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 100 == 0:\n",
    "            avg_loss = train_loss / (batch_idx + 1)\n",
    "        pass\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    \n",
    "    #print(datetime.datetime.now())\n",
    "    # Validate on the validation dataset\n",
    "    #print(\"Validation...\")\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    # Disable gradient during validation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # Copy inputs to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Generate output from the DNN.\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Calculate predicted labels\n",
    "            _, predicted = outputs.max(1)\n",
    "            # Calculate accuracy\n",
    "            total_examples += inputs.size(0)\n",
    "            correct_examples += torch.eq(targets, predicted).sum().item()\n",
    "            val_loss += loss\n",
    "\n",
    "    avg_loss = val_loss / len(valloader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "    \n",
    "    # For graphing\n",
    "    val_losses.append(avg_loss)\n",
    "    epochs.append(i)\n",
    "    validation_accuracies.append(avg_acc)\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    Assignment 4(b)\n",
    "    Learning rate is an important hyperparameter to tune. Specify a \n",
    "    learning rate decay policy and apply it in your training process. \n",
    "    Briefly describe its impact on the learning curveduring your \n",
    "    training process.    \n",
    "    Reference learning rate schedule: \n",
    "    decay 0.98 for every 2 epochs. You may tune this parameter but \n",
    "    minimal gain will be achieved.\n",
    "    Assignment 4(c)\n",
    "    As we can see from above, hyperparameter optimization is critical \n",
    "    to obtain a good performance of DNN models. Try to fine-tune the \n",
    "    model to over 70% accuracy. You may also increase the number of \n",
    "    epochs to up to 100 during the process. Briefly describe what you \n",
    "    have tried to improve the performance of the LeNet-5 model.\n",
    "    \"\"\"\n",
    "    DECAY_EPOCHS = 2\n",
    "    DECAY = 0.98\n",
    "    if i % DECAY_EPOCHS == 0 and i != 0:\n",
    "        current_learning_rate *= DECAY\n",
    "        for param_group in optimizer.param_groups:\n",
    "            # Assign the learning rate parameter\n",
    "            param_group['lr'] = current_learning_rate\n",
    "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
    "    \n",
    "    # Save for checkpoint\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "        if not os.path.exists(CHECKPOINT_PATH):\n",
    "            os.makedirs(CHECKPOINT_PATH)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'net': net.state_dict(),\n",
    "                 'epoch': i,\n",
    "                 'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_PATH, 'model.h5'))\n",
    "\n",
    "print(\"Optimization finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max validation acc: 0.8462  at Epoch:  90\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "tmp = []\n",
    "for i in val_losses:\n",
    "    tmp.append(i)\n",
    "val_losses = tmp\n",
    "\n",
    "tmp = []\n",
    "for i in train_losses:\n",
    "    tmp.append(i)\n",
    "train_losses = tmp\n",
    "\n",
    "epochs = []\n",
    "for i in range(len(val_losses)):\n",
    "    epochs.append(i)\n",
    "\n",
    "    \n",
    "val, idx = max((val, idx) for (idx, val) in enumerate(validation_accuracies))\n",
    "print(\"Max validation acc:\", val, \" at Epoch: \", idx)\n",
    "    \n",
    "#plt.plot(epochs, val_losses, label=\"val_losses\") \n",
    "plt.plot(epochs, train_losses, label=\"train_losses\") \n",
    "plt.legend()\n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Loss') \n",
    "plt.title('Loss curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.testloader import TEST_SET\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "testset = TEST_SET(root=DATAROOT, train=False, download=True, transform=transform_train)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=1)\n",
    "\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # Copy inputs to device         \n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output from the DNN.\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # Calculate predicted labels\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        predicted_np = predicted.data.cpu().numpy()\n",
    "        for i in predicted_np:\n",
    "            results.append(i)\n",
    "# len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"Id\", \"Category\"]\n",
    "\n",
    "out_to_file = []\n",
    "\n",
    "out_to_file.append(header)\n",
    "\n",
    "for i in range(len(results)):\n",
    "    tmp = [i, results[i]]\n",
    "    out_to_file.append(tmp)\n",
    "\n",
    "#print(out_to_file)\n",
    "\n",
    "np_out = np.array(out_to_file)\n",
    "\n",
    "np.savetxt('output.csv', np_out, fmt='%s', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
