{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Import pytorch dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# You cannot change this line.\n",
    "from tools.dataloader import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network module: VGG16\n",
    "class Vgg16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vgg16, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3,64,3) #(in, out, kernel)\n",
    "#         self.conv2 = nn.Conv2d(64,64,3)\n",
    "#         self.pool1 = nn.MaxPool2d(2,2) #(kernel, stride)\n",
    "#         self.conv3 = nn.Conv2d(64,128,3)\n",
    "#         self.conv4 = nn.Conv2d(128,128,3)\n",
    "#         self.pool2 = nn.MaxPool2d(2,2)\n",
    "#         self.conv5 = nn.Conv2d(128,256,3)\n",
    "#         self.conv6 = nn.Conv2d(256,256,3)\n",
    "#         self.fc1 = nn.Linear(1 * 1 * 256, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3,64,3) #(in, out, kernel)\n",
    "        self.conv1bn = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64,64,3)\n",
    "        self.conv2bn = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(2,2) #(kernel, stride)\n",
    "        self.conv3 = nn.Conv2d(64,128,3)\n",
    "        self.conv3bn = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.conv4 = nn.Conv2d(128,256,3)\n",
    "        self.conv4bn = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(2 * 2 * 256, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1bn(self.conv1(x)))\n",
    "       \n",
    "        x = F.relu(self.conv2bn(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv3bn(self.conv3(x)))\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv4bn(self.conv4(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        \n",
    "        #x = F.relu(self.conv6(x))\n",
    "        \n",
    "        \n",
    "        #x = F.relu(self.fc1(x.view(-1, 1 * 1 * 256)))\n",
    "        x = F.relu(self.fc1(x.view(-1, 2 * 2 * 256)))\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([64, 3, 3, 3])\n",
      "conv1.bias torch.Size([64])\n",
      "conv1bn.weight torch.Size([64])\n",
      "conv1bn.bias torch.Size([64])\n",
      "conv2.weight torch.Size([64, 64, 3, 3])\n",
      "conv2.bias torch.Size([64])\n",
      "conv2bn.weight torch.Size([64])\n",
      "conv2bn.bias torch.Size([64])\n",
      "conv3.weight torch.Size([128, 64, 3, 3])\n",
      "conv3.bias torch.Size([128])\n",
      "conv3bn.weight torch.Size([128])\n",
      "conv3bn.bias torch.Size([128])\n",
      "conv4.weight torch.Size([256, 128, 3, 3])\n",
      "conv4.bias torch.Size([256])\n",
      "conv4bn.weight torch.Size([256])\n",
      "conv4bn.bias torch.Size([256])\n",
      "fc1.weight torch.Size([120, 1024])\n",
      "fc1.bias torch.Size([120])\n",
      "fc2.weight torch.Size([84, 120])\n",
      "fc2.bias torch.Size([84])\n",
      "fc3.weight torch.Size([10, 84])\n",
      "fc3.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Print some parameters for checking\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = Vgg16()\n",
    "net = net.to(device)\n",
    "testArray = torch.zeros([128,3,32,32])\n",
    "x = testArray.to(device)\n",
    "y = net(x)\n",
    "for name, p in net.named_parameters():\n",
    "    print(name, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting some hyperparameters\n",
    "\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 100\n",
    "INITIAL_LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "REG = 1e-4\n",
    "EPOCHS = 150\n",
    "DATAROOT = \"./data\"\n",
    "CHECKPOINT_PATH = \"./saved_model_vgg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=2, hue=0.2), #contrast=2, saturation=2,\n",
    "    transforms.RandomCrop([32, 32], padding=2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/cifar10_trainval.tar.gz\n",
      "Extracting ./data/cifar10_trainval.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data/cifar10_trainval.tar.gz\n",
      "Extracting ./data/cifar10_trainval.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Call the dataset Loader\n",
    "trainset = CIFAR10(root=DATAROOT, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=1)\n",
    "valset = CIFAR10(root=DATAROOT, train=False, download=True, transform=transform_val)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on GPU...\n"
     ]
    }
   ],
   "source": [
    "# Specify the device for computation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# net = Vgg16()\n",
    "# net = net.to(device)\n",
    "if device =='cuda':\n",
    "    print(\"Train on GPU...\")\n",
    "else:\n",
    "    print(\"Train on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded checkpoint: ./saved_model_vgg/model.h5\n",
      "Starting from epoch 16 \n",
      "Starting from learning rate 0.086813:\n"
     ]
    }
   ],
   "source": [
    "# FLAG for loading the pretrained model\n",
    "TRAIN_FROM_SCRATCH = False\n",
    "# Code for loading checkpoint and recover epoch id.\n",
    "CKPT_PATH = \"./saved_model_vgg/model.h5\"\n",
    "def get_checkpoint(ckpt_path):\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return ckpt\n",
    "\n",
    "ckpt = get_checkpoint(CKPT_PATH)\n",
    "if ckpt is None or TRAIN_FROM_SCRATCH:\n",
    "    if not TRAIN_FROM_SCRATCH:\n",
    "        print(\"Checkpoint not found.\")\n",
    "    print(\"Training from scratch ...\")\n",
    "    start_epoch = 0\n",
    "    current_learning_rate = INITIAL_LR\n",
    "else:\n",
    "    print(\"Successfully loaded checkpoint: %s\" %CKPT_PATH)\n",
    "    net.load_state_dict(ckpt['net'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    current_learning_rate = ckpt['lr']\n",
    "    print(\"Starting from epoch %d \" %start_epoch)\n",
    "\n",
    "print(\"Starting from learning rate %f:\" %current_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss function and specify regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Add optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper variables for graphing\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "validation_accuracies = []\n",
    "epochs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:\n",
      "352\n",
      "Training loss: 0.7592, Training accuracy: 0.7374\n",
      "Validation loss: 0.6400, Validation accuracy: 0.7810\n",
      "Current learning rate has decayed to 0.085076\n",
      "Saving ...\n",
      "Epoch 17:\n",
      "352\n",
      "Training loss: 0.7361, Training accuracy: 0.7465\n",
      "Validation loss: 0.6501, Validation accuracy: 0.7860\n",
      "Saving ...\n",
      "Epoch 18:\n",
      "352\n",
      "Training loss: 0.7169, Training accuracy: 0.7546\n",
      "Validation loss: 0.5855, Validation accuracy: 0.7974\n",
      "Current learning rate has decayed to 0.083375\n",
      "Saving ...\n",
      "Epoch 19:\n",
      "352\n",
      "Training loss: 0.7011, Training accuracy: 0.7585\n",
      "Validation loss: 0.6091, Validation accuracy: 0.7940\n",
      "Epoch 20:\n",
      "352\n",
      "Training loss: 0.7001, Training accuracy: 0.7600\n",
      "Validation loss: 0.5763, Validation accuracy: 0.8112\n",
      "Current learning rate has decayed to 0.081707\n",
      "Saving ...\n",
      "Epoch 21:\n",
      "352\n",
      "Training loss: 0.6761, Training accuracy: 0.7658\n",
      "Validation loss: 0.5713, Validation accuracy: 0.8040\n",
      "Epoch 22:\n",
      "352\n",
      "Training loss: 0.6698, Training accuracy: 0.7703\n",
      "Validation loss: 0.5801, Validation accuracy: 0.8026\n",
      "Current learning rate has decayed to 0.080073\n",
      "Epoch 23:\n",
      "352\n",
      "Training loss: 0.6635, Training accuracy: 0.7722\n",
      "Validation loss: 0.6133, Validation accuracy: 0.7998\n",
      "Epoch 24:\n",
      "352\n",
      "Training loss: 0.6553, Training accuracy: 0.7767\n",
      "Validation loss: 0.6281, Validation accuracy: 0.7928\n",
      "Current learning rate has decayed to 0.078472\n",
      "Epoch 25:\n",
      "352\n",
      "Training loss: 0.6420, Training accuracy: 0.7805\n",
      "Validation loss: 0.5667, Validation accuracy: 0.8090\n",
      "Epoch 26:\n",
      "352\n",
      "Training loss: 0.6383, Training accuracy: 0.7796\n",
      "Validation loss: 0.5811, Validation accuracy: 0.8032\n",
      "Current learning rate has decayed to 0.076902\n",
      "Epoch 27:\n",
      "352\n",
      "Training loss: 0.6266, Training accuracy: 0.7852\n",
      "Validation loss: 0.5501, Validation accuracy: 0.8208\n",
      "Saving ...\n",
      "Epoch 28:\n",
      "352\n",
      "Training loss: 0.6186, Training accuracy: 0.7873\n",
      "Validation loss: 0.6185, Validation accuracy: 0.7814\n",
      "Current learning rate has decayed to 0.075364\n",
      "Epoch 29:\n",
      "352\n",
      "Training loss: 0.6074, Training accuracy: 0.7890\n",
      "Validation loss: 0.5732, Validation accuracy: 0.8100\n",
      "Epoch 30:\n",
      "352\n",
      "Training loss: 0.6060, Training accuracy: 0.7912\n",
      "Validation loss: 0.5422, Validation accuracy: 0.8184\n",
      "Current learning rate has decayed to 0.073857\n",
      "Epoch 31:\n",
      "352\n",
      "Training loss: 0.5989, Training accuracy: 0.7949\n",
      "Validation loss: 0.5338, Validation accuracy: 0.8180\n",
      "Epoch 32:\n",
      "352\n",
      "Training loss: 0.5914, Training accuracy: 0.7948\n",
      "Validation loss: 0.5632, Validation accuracy: 0.8174\n",
      "Current learning rate has decayed to 0.072380\n",
      "Epoch 33:\n",
      "352\n",
      "Training loss: 0.5914, Training accuracy: 0.7970\n",
      "Validation loss: 0.5400, Validation accuracy: 0.8204\n",
      "Epoch 34:\n",
      "352\n",
      "Training loss: 0.5841, Training accuracy: 0.7976\n",
      "Validation loss: 0.5439, Validation accuracy: 0.8192\n",
      "Current learning rate has decayed to 0.070932\n",
      "Epoch 35:\n",
      "352\n",
      "Training loss: 0.5640, Training accuracy: 0.8079\n",
      "Validation loss: 0.5369, Validation accuracy: 0.8238\n",
      "Saving ...\n",
      "Epoch 36:\n",
      "352\n",
      "Training loss: 0.5748, Training accuracy: 0.7998\n",
      "Validation loss: 0.5300, Validation accuracy: 0.8240\n",
      "Current learning rate has decayed to 0.069514\n",
      "Saving ...\n",
      "Epoch 37:\n",
      "352\n",
      "Training loss: 0.5672, Training accuracy: 0.8037\n",
      "Validation loss: 0.5095, Validation accuracy: 0.8278\n",
      "Saving ...\n",
      "Epoch 38:\n",
      "352\n",
      "Training loss: 0.5544, Training accuracy: 0.8074\n",
      "Validation loss: 0.5513, Validation accuracy: 0.8156\n",
      "Current learning rate has decayed to 0.068123\n",
      "Epoch 39:\n",
      "352\n",
      "Training loss: 0.5530, Training accuracy: 0.8083\n",
      "Validation loss: 0.5182, Validation accuracy: 0.8216\n",
      "Epoch 40:\n",
      "352\n",
      "Training loss: 0.5429, Training accuracy: 0.8114\n",
      "Validation loss: 0.5098, Validation accuracy: 0.8296\n",
      "Current learning rate has decayed to 0.066761\n",
      "Saving ...\n",
      "Epoch 41:\n",
      "352\n",
      "Training loss: 0.5306, Training accuracy: 0.8171\n",
      "Validation loss: 0.5625, Validation accuracy: 0.8176\n",
      "Epoch 42:\n",
      "352\n",
      "Training loss: 0.5316, Training accuracy: 0.8158\n",
      "Validation loss: 0.5406, Validation accuracy: 0.8274\n",
      "Current learning rate has decayed to 0.065426\n",
      "Epoch 43:\n",
      "352\n",
      "Training loss: 0.5419, Training accuracy: 0.8128\n",
      "Validation loss: 0.4871, Validation accuracy: 0.8324\n",
      "Saving ...\n",
      "Epoch 44:\n",
      "352\n",
      "Training loss: 0.5237, Training accuracy: 0.8168\n",
      "Validation loss: 0.5519, Validation accuracy: 0.8236\n",
      "Current learning rate has decayed to 0.064117\n",
      "Epoch 45:\n",
      "352\n",
      "Training loss: 0.5220, Training accuracy: 0.8193\n",
      "Validation loss: 0.4827, Validation accuracy: 0.8410\n",
      "Saving ...\n",
      "Epoch 46:\n",
      "352\n",
      "Training loss: 0.5129, Training accuracy: 0.8228\n",
      "Validation loss: 0.5341, Validation accuracy: 0.8290\n",
      "Current learning rate has decayed to 0.062835\n",
      "Epoch 47:\n",
      "352\n",
      "Training loss: 0.5119, Training accuracy: 0.8239\n",
      "Validation loss: 0.4881, Validation accuracy: 0.8382\n",
      "Epoch 48:\n",
      "352\n",
      "Training loss: 0.5129, Training accuracy: 0.8221\n",
      "Validation loss: 0.5191, Validation accuracy: 0.8282\n",
      "Current learning rate has decayed to 0.061578\n",
      "Epoch 49:\n",
      "352\n",
      "Training loss: 0.4995, Training accuracy: 0.8248\n",
      "Validation loss: 0.5367, Validation accuracy: 0.8268\n",
      "Epoch 50:\n",
      "352\n",
      "Training loss: 0.4992, Training accuracy: 0.8254\n",
      "Validation loss: 0.5015, Validation accuracy: 0.8378\n",
      "Current learning rate has decayed to 0.060346\n",
      "Epoch 51:\n",
      "352\n",
      "Training loss: 0.4921, Training accuracy: 0.8286\n",
      "Validation loss: 0.4771, Validation accuracy: 0.8414\n",
      "Saving ...\n",
      "Epoch 52:\n",
      "352\n",
      "Training loss: 0.4981, Training accuracy: 0.8279\n",
      "Validation loss: 0.5149, Validation accuracy: 0.8368\n",
      "Current learning rate has decayed to 0.059140\n",
      "Epoch 53:\n",
      "352\n",
      "Training loss: 0.4926, Training accuracy: 0.8297\n",
      "Validation loss: 0.4987, Validation accuracy: 0.8376\n",
      "Epoch 54:\n",
      "352\n",
      "Training loss: 0.4851, Training accuracy: 0.8326\n",
      "Validation loss: 0.4950, Validation accuracy: 0.8434\n",
      "Current learning rate has decayed to 0.057957\n",
      "Saving ...\n",
      "Epoch 55:\n",
      "352\n",
      "Training loss: 0.4845, Training accuracy: 0.8330\n",
      "Validation loss: 0.5164, Validation accuracy: 0.8332\n",
      "Epoch 56:\n",
      "352\n"
     ]
    }
   ],
   "source": [
    "# Start the training/validation process\n",
    "# The process should take about 5 minutes on a GTX 1070-Ti\n",
    "# if the code is written efficiently.\n",
    "global_step = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "for i in range(start_epoch, EPOCHS):\n",
    "    #print(datetime.datetime.now())\n",
    "    # Switch to train mode\n",
    "    net.train()\n",
    "    print(\"Epoch %d:\" %i)\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    # Train the training dataset for 1 epoch.\n",
    "    print(len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # Copy inputs to device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # Printing initial loss \n",
    "        if i == 0 and batch_idx == 0:\n",
    "            print(\"initial loss: \",  loss)\n",
    "        \n",
    "        # Now backward loss\n",
    "        loss.backward()\n",
    "        # Apply gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate predicted labels\n",
    "        _, predicted = outputs.max(1)\n",
    "        # Calculate accuracy\n",
    "        total_examples += inputs.size(0)\n",
    "        correct_examples += torch.eq(targets, predicted).sum().item()\n",
    "\n",
    "        train_loss += loss\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 100 == 0:\n",
    "            avg_loss = train_loss / (batch_idx + 1)\n",
    "        pass\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    \n",
    "    #print(datetime.datetime.now())\n",
    "    # Validate on the validation dataset\n",
    "    #print(\"Validation...\")\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    # Disable gradient during validation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # Copy inputs to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Generate output from the DNN.\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Calculate predicted labels\n",
    "            _, predicted = outputs.max(1)\n",
    "            # Calculate accuracy\n",
    "            total_examples += inputs.size(0)\n",
    "            correct_examples += torch.eq(targets, predicted).sum().item()\n",
    "            val_loss += loss\n",
    "\n",
    "    avg_loss = val_loss / len(valloader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "    \n",
    "    # For graphing\n",
    "    val_losses.append(avg_loss)\n",
    "    epochs.append(i)\n",
    "    validation_accuracies.append(avg_acc)\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    Assignment 4(b)\n",
    "    Learning rate is an important hyperparameter to tune. Specify a \n",
    "    learning rate decay policy and apply it in your training process. \n",
    "    Briefly describe its impact on the learning curveduring your \n",
    "    training process.    \n",
    "    Reference learning rate schedule: \n",
    "    decay 0.98 for every 2 epochs. You may tune this parameter but \n",
    "    minimal gain will be achieved.\n",
    "    Assignment 4(c)\n",
    "    As we can see from above, hyperparameter optimization is critical \n",
    "    to obtain a good performance of DNN models. Try to fine-tune the \n",
    "    model to over 70% accuracy. You may also increase the number of \n",
    "    epochs to up to 100 during the process. Briefly describe what you \n",
    "    have tried to improve the performance of the LeNet-5 model.\n",
    "    \"\"\"\n",
    "    DECAY_EPOCHS = 2\n",
    "    DECAY = 0.98\n",
    "    if i % DECAY_EPOCHS == 0 and i != 0:\n",
    "        current_learning_rate *= DECAY\n",
    "        for param_group in optimizer.param_groups:\n",
    "            # Assign the learning rate parameter\n",
    "            param_group['lr'] = current_learning_rate\n",
    "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
    "    \n",
    "    # Save for checkpoint\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "        if not os.path.exists(CHECKPOINT_PATH):\n",
    "            os.makedirs(CHECKPOINT_PATH)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'net': net.state_dict(),\n",
    "                 'epoch': i,\n",
    "                 'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_PATH, 'model.h5'))\n",
    "\n",
    "print(\"Optimization finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "tmp = []\n",
    "for i in val_losses:\n",
    "    tmp.append(i)\n",
    "val_losses = tmp\n",
    "\n",
    "tmp = []\n",
    "for i in train_losses:\n",
    "    tmp.append(i)\n",
    "train_losses = tmp\n",
    "\n",
    "epochs = []\n",
    "for i in range(len(val_losses)):\n",
    "    epochs.append(i)\n",
    "\n",
    "    \n",
    "val, idx = max((val, idx) for (idx, val) in enumerate(validation_accuracies))\n",
    "print(\"Max validation acc:\", val, \" at Epoch: \", idx)\n",
    "    \n",
    "#plt.plot(epochs, val_losses, label=\"val_losses\") \n",
    "plt.plot(epochs, train_losses, label=\"train_losses\") \n",
    "plt.legend()\n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Loss') \n",
    "plt.title('Loss curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.testloader import TEST_SET\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "testset = TEST_SET(root=DATAROOT, train=False, transform=transform_train)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=1)\n",
    "\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # Copy inputs to device         \n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output from the DNN.\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # Calculate predicted labels\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        predicted_np = predicted.data.cpu().numpy()\n",
    "        for i in predicted_np:\n",
    "            results.append(i)\n",
    "# len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"Id\", \"Category\"]\n",
    "\n",
    "out_to_file = []\n",
    "\n",
    "out_to_file.append(header)\n",
    "\n",
    "for i in range(len(results)):\n",
    "    tmp = [i, results[i]]\n",
    "    out_to_file.append(tmp)\n",
    "\n",
    "#print(out_to_file)\n",
    "\n",
    "np_out = np.array(out_to_file)\n",
    "\n",
    "np.savetxt('output.csv', np_out, fmt='%s', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
